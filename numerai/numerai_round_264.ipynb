{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "numerai_round_264.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOVaUpg+Mp0g8trwxfhJqh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuj0456/deep_learning_for_coders/blob/master/numerai/numerai_round_264.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZq3_uLGO2sk",
        "outputId": "8af6caa7-08cc-42fb-c6a7-c82031e6ffe8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cq2-WgdPOT_",
        "outputId": "810f52e0-de70-45b0-e379-e270c409cfe9"
      },
      "source": [
        "!unzip /content/drive/MyDrive/ML/numerai/numerai_datasets.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/ML/numerai/numerai_datasets.zip\n",
            "replace numerai_training_data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGfUENAKPtIt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb \n",
        "import tensorflow as tf\n",
        "import lightgbm as lgbm\n",
        "from xgboost import DMatrix\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "xU0T_uo3RtK8",
        "outputId": "d01d1317-19d1-4035-ee12-44248b54fdc2"
      },
      "source": [
        "df_train = pd.read_csv(\"numerai_training_data.csv\")\n",
        "df_train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n000315175b67977</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n0014af834a96cdd</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n001c93979ac41d4</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n0034e4143f22a13</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n00679d1a636062f</td>\n",
              "      <td>era1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id   era data_type  ...  feature_wisdom45  feature_wisdom46  target\n",
              "0  n000315175b67977  era1     train  ...              0.50              0.75    0.50\n",
              "1  n0014af834a96cdd  era1     train  ...              0.25              1.00    0.25\n",
              "2  n001c93979ac41d4  era1     train  ...              0.25              0.75    0.25\n",
              "3  n0034e4143f22a13  era1     train  ...              1.00              1.00    0.25\n",
              "4  n00679d1a636062f  era1     train  ...              0.25              0.75    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "pqXUZYBMSJ2Q",
        "outputId": "04588067-a1a9-4c6a-fc63-8a0266357a87"
      },
      "source": [
        "df_test = pd.read_csv(\"numerai_tournament_data.csv\")\n",
        "df_test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom8</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>era121</td>\n",
              "      <td>validation</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 314 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     era  ... feature_wisdom46  target\n",
              "0  n0003aa52cab36c2  era121  ...             0.00    0.25\n",
              "1  n000920ed083903f  era121  ...             0.50    0.50\n",
              "2  n0038e640522c4a6  era121  ...             0.00    1.00\n",
              "3  n004ac94a87dc54b  era121  ...             0.25    0.50\n",
              "4  n0052fe97ea0c05f  era121  ...             1.00    0.75\n",
              "\n",
              "[5 rows x 314 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j90FD7LrGwD4",
        "outputId": "83bd1b8d-743b-432e-aad5-c8b9d270227d"
      },
      "source": [
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(501808, 314)\n",
            "(1725602, 314)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emic3zRjHCsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6882e680-d029-47de-cbcd-80f6fcaf26fe"
      },
      "source": [
        "# There's 310 features\n",
        "features = [c for c in df_train if c.startswith(\"feature\")]\n",
        "df_train[\"erano\"] = df_train.era.str.slice(3).astype(int)\n",
        "eras = df_train.erano\n",
        "target = \"target\"\n",
        "len(features)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "310"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlaZsny4crxh",
        "outputId": "235d59e3-2f0f-4ebb-b21b-b61bae5b41c3"
      },
      "source": [
        "# The features are grouped together into 6 types\n",
        "feature_groups = {g: [c for c in df_train if c.startswith(f\"feature_{g}\")]\n",
        "    for g in [\"intelligence\", \"wisdom\", \"charisma\", \"dexterity\", \"strength\", \"constitution\"]\n",
        "}\n",
        "feature_groups"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'charisma': ['feature_charisma1',\n",
              "  'feature_charisma2',\n",
              "  'feature_charisma3',\n",
              "  'feature_charisma4',\n",
              "  'feature_charisma5',\n",
              "  'feature_charisma6',\n",
              "  'feature_charisma7',\n",
              "  'feature_charisma8',\n",
              "  'feature_charisma9',\n",
              "  'feature_charisma10',\n",
              "  'feature_charisma11',\n",
              "  'feature_charisma12',\n",
              "  'feature_charisma13',\n",
              "  'feature_charisma14',\n",
              "  'feature_charisma15',\n",
              "  'feature_charisma16',\n",
              "  'feature_charisma17',\n",
              "  'feature_charisma18',\n",
              "  'feature_charisma19',\n",
              "  'feature_charisma20',\n",
              "  'feature_charisma21',\n",
              "  'feature_charisma22',\n",
              "  'feature_charisma23',\n",
              "  'feature_charisma24',\n",
              "  'feature_charisma25',\n",
              "  'feature_charisma26',\n",
              "  'feature_charisma27',\n",
              "  'feature_charisma28',\n",
              "  'feature_charisma29',\n",
              "  'feature_charisma30',\n",
              "  'feature_charisma31',\n",
              "  'feature_charisma32',\n",
              "  'feature_charisma33',\n",
              "  'feature_charisma34',\n",
              "  'feature_charisma35',\n",
              "  'feature_charisma36',\n",
              "  'feature_charisma37',\n",
              "  'feature_charisma38',\n",
              "  'feature_charisma39',\n",
              "  'feature_charisma40',\n",
              "  'feature_charisma41',\n",
              "  'feature_charisma42',\n",
              "  'feature_charisma43',\n",
              "  'feature_charisma44',\n",
              "  'feature_charisma45',\n",
              "  'feature_charisma46',\n",
              "  'feature_charisma47',\n",
              "  'feature_charisma48',\n",
              "  'feature_charisma49',\n",
              "  'feature_charisma50',\n",
              "  'feature_charisma51',\n",
              "  'feature_charisma52',\n",
              "  'feature_charisma53',\n",
              "  'feature_charisma54',\n",
              "  'feature_charisma55',\n",
              "  'feature_charisma56',\n",
              "  'feature_charisma57',\n",
              "  'feature_charisma58',\n",
              "  'feature_charisma59',\n",
              "  'feature_charisma60',\n",
              "  'feature_charisma61',\n",
              "  'feature_charisma62',\n",
              "  'feature_charisma63',\n",
              "  'feature_charisma64',\n",
              "  'feature_charisma65',\n",
              "  'feature_charisma66',\n",
              "  'feature_charisma67',\n",
              "  'feature_charisma68',\n",
              "  'feature_charisma69',\n",
              "  'feature_charisma70',\n",
              "  'feature_charisma71',\n",
              "  'feature_charisma72',\n",
              "  'feature_charisma73',\n",
              "  'feature_charisma74',\n",
              "  'feature_charisma75',\n",
              "  'feature_charisma76',\n",
              "  'feature_charisma77',\n",
              "  'feature_charisma78',\n",
              "  'feature_charisma79',\n",
              "  'feature_charisma80',\n",
              "  'feature_charisma81',\n",
              "  'feature_charisma82',\n",
              "  'feature_charisma83',\n",
              "  'feature_charisma84',\n",
              "  'feature_charisma85',\n",
              "  'feature_charisma86'],\n",
              " 'constitution': ['feature_constitution1',\n",
              "  'feature_constitution2',\n",
              "  'feature_constitution3',\n",
              "  'feature_constitution4',\n",
              "  'feature_constitution5',\n",
              "  'feature_constitution6',\n",
              "  'feature_constitution7',\n",
              "  'feature_constitution8',\n",
              "  'feature_constitution9',\n",
              "  'feature_constitution10',\n",
              "  'feature_constitution11',\n",
              "  'feature_constitution12',\n",
              "  'feature_constitution13',\n",
              "  'feature_constitution14',\n",
              "  'feature_constitution15',\n",
              "  'feature_constitution16',\n",
              "  'feature_constitution17',\n",
              "  'feature_constitution18',\n",
              "  'feature_constitution19',\n",
              "  'feature_constitution20',\n",
              "  'feature_constitution21',\n",
              "  'feature_constitution22',\n",
              "  'feature_constitution23',\n",
              "  'feature_constitution24',\n",
              "  'feature_constitution25',\n",
              "  'feature_constitution26',\n",
              "  'feature_constitution27',\n",
              "  'feature_constitution28',\n",
              "  'feature_constitution29',\n",
              "  'feature_constitution30',\n",
              "  'feature_constitution31',\n",
              "  'feature_constitution32',\n",
              "  'feature_constitution33',\n",
              "  'feature_constitution34',\n",
              "  'feature_constitution35',\n",
              "  'feature_constitution36',\n",
              "  'feature_constitution37',\n",
              "  'feature_constitution38',\n",
              "  'feature_constitution39',\n",
              "  'feature_constitution40',\n",
              "  'feature_constitution41',\n",
              "  'feature_constitution42',\n",
              "  'feature_constitution43',\n",
              "  'feature_constitution44',\n",
              "  'feature_constitution45',\n",
              "  'feature_constitution46',\n",
              "  'feature_constitution47',\n",
              "  'feature_constitution48',\n",
              "  'feature_constitution49',\n",
              "  'feature_constitution50',\n",
              "  'feature_constitution51',\n",
              "  'feature_constitution52',\n",
              "  'feature_constitution53',\n",
              "  'feature_constitution54',\n",
              "  'feature_constitution55',\n",
              "  'feature_constitution56',\n",
              "  'feature_constitution57',\n",
              "  'feature_constitution58',\n",
              "  'feature_constitution59',\n",
              "  'feature_constitution60',\n",
              "  'feature_constitution61',\n",
              "  'feature_constitution62',\n",
              "  'feature_constitution63',\n",
              "  'feature_constitution64',\n",
              "  'feature_constitution65',\n",
              "  'feature_constitution66',\n",
              "  'feature_constitution67',\n",
              "  'feature_constitution68',\n",
              "  'feature_constitution69',\n",
              "  'feature_constitution70',\n",
              "  'feature_constitution71',\n",
              "  'feature_constitution72',\n",
              "  'feature_constitution73',\n",
              "  'feature_constitution74',\n",
              "  'feature_constitution75',\n",
              "  'feature_constitution76',\n",
              "  'feature_constitution77',\n",
              "  'feature_constitution78',\n",
              "  'feature_constitution79',\n",
              "  'feature_constitution80',\n",
              "  'feature_constitution81',\n",
              "  'feature_constitution82',\n",
              "  'feature_constitution83',\n",
              "  'feature_constitution84',\n",
              "  'feature_constitution85',\n",
              "  'feature_constitution86',\n",
              "  'feature_constitution87',\n",
              "  'feature_constitution88',\n",
              "  'feature_constitution89',\n",
              "  'feature_constitution90',\n",
              "  'feature_constitution91',\n",
              "  'feature_constitution92',\n",
              "  'feature_constitution93',\n",
              "  'feature_constitution94',\n",
              "  'feature_constitution95',\n",
              "  'feature_constitution96',\n",
              "  'feature_constitution97',\n",
              "  'feature_constitution98',\n",
              "  'feature_constitution99',\n",
              "  'feature_constitution100',\n",
              "  'feature_constitution101',\n",
              "  'feature_constitution102',\n",
              "  'feature_constitution103',\n",
              "  'feature_constitution104',\n",
              "  'feature_constitution105',\n",
              "  'feature_constitution106',\n",
              "  'feature_constitution107',\n",
              "  'feature_constitution108',\n",
              "  'feature_constitution109',\n",
              "  'feature_constitution110',\n",
              "  'feature_constitution111',\n",
              "  'feature_constitution112',\n",
              "  'feature_constitution113',\n",
              "  'feature_constitution114'],\n",
              " 'dexterity': ['feature_dexterity1',\n",
              "  'feature_dexterity2',\n",
              "  'feature_dexterity3',\n",
              "  'feature_dexterity4',\n",
              "  'feature_dexterity5',\n",
              "  'feature_dexterity6',\n",
              "  'feature_dexterity7',\n",
              "  'feature_dexterity8',\n",
              "  'feature_dexterity9',\n",
              "  'feature_dexterity10',\n",
              "  'feature_dexterity11',\n",
              "  'feature_dexterity12',\n",
              "  'feature_dexterity13',\n",
              "  'feature_dexterity14'],\n",
              " 'intelligence': ['feature_intelligence1',\n",
              "  'feature_intelligence2',\n",
              "  'feature_intelligence3',\n",
              "  'feature_intelligence4',\n",
              "  'feature_intelligence5',\n",
              "  'feature_intelligence6',\n",
              "  'feature_intelligence7',\n",
              "  'feature_intelligence8',\n",
              "  'feature_intelligence9',\n",
              "  'feature_intelligence10',\n",
              "  'feature_intelligence11',\n",
              "  'feature_intelligence12'],\n",
              " 'strength': ['feature_strength1',\n",
              "  'feature_strength2',\n",
              "  'feature_strength3',\n",
              "  'feature_strength4',\n",
              "  'feature_strength5',\n",
              "  'feature_strength6',\n",
              "  'feature_strength7',\n",
              "  'feature_strength8',\n",
              "  'feature_strength9',\n",
              "  'feature_strength10',\n",
              "  'feature_strength11',\n",
              "  'feature_strength12',\n",
              "  'feature_strength13',\n",
              "  'feature_strength14',\n",
              "  'feature_strength15',\n",
              "  'feature_strength16',\n",
              "  'feature_strength17',\n",
              "  'feature_strength18',\n",
              "  'feature_strength19',\n",
              "  'feature_strength20',\n",
              "  'feature_strength21',\n",
              "  'feature_strength22',\n",
              "  'feature_strength23',\n",
              "  'feature_strength24',\n",
              "  'feature_strength25',\n",
              "  'feature_strength26',\n",
              "  'feature_strength27',\n",
              "  'feature_strength28',\n",
              "  'feature_strength29',\n",
              "  'feature_strength30',\n",
              "  'feature_strength31',\n",
              "  'feature_strength32',\n",
              "  'feature_strength33',\n",
              "  'feature_strength34',\n",
              "  'feature_strength35',\n",
              "  'feature_strength36',\n",
              "  'feature_strength37',\n",
              "  'feature_strength38'],\n",
              " 'wisdom': ['feature_wisdom1',\n",
              "  'feature_wisdom2',\n",
              "  'feature_wisdom3',\n",
              "  'feature_wisdom4',\n",
              "  'feature_wisdom5',\n",
              "  'feature_wisdom6',\n",
              "  'feature_wisdom7',\n",
              "  'feature_wisdom8',\n",
              "  'feature_wisdom9',\n",
              "  'feature_wisdom10',\n",
              "  'feature_wisdom11',\n",
              "  'feature_wisdom12',\n",
              "  'feature_wisdom13',\n",
              "  'feature_wisdom14',\n",
              "  'feature_wisdom15',\n",
              "  'feature_wisdom16',\n",
              "  'feature_wisdom17',\n",
              "  'feature_wisdom18',\n",
              "  'feature_wisdom19',\n",
              "  'feature_wisdom20',\n",
              "  'feature_wisdom21',\n",
              "  'feature_wisdom22',\n",
              "  'feature_wisdom23',\n",
              "  'feature_wisdom24',\n",
              "  'feature_wisdom25',\n",
              "  'feature_wisdom26',\n",
              "  'feature_wisdom27',\n",
              "  'feature_wisdom28',\n",
              "  'feature_wisdom29',\n",
              "  'feature_wisdom30',\n",
              "  'feature_wisdom31',\n",
              "  'feature_wisdom32',\n",
              "  'feature_wisdom33',\n",
              "  'feature_wisdom34',\n",
              "  'feature_wisdom35',\n",
              "  'feature_wisdom36',\n",
              "  'feature_wisdom37',\n",
              "  'feature_wisdom38',\n",
              "  'feature_wisdom39',\n",
              "  'feature_wisdom40',\n",
              "  'feature_wisdom41',\n",
              "  'feature_wisdom42',\n",
              "  'feature_wisdom43',\n",
              "  'feature_wisdom44',\n",
              "  'feature_wisdom45',\n",
              "  'feature_wisdom46']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkq4uy9Fc0xR"
      },
      "source": [
        "# The models should be scored based on the rank-correlation (spearman) with the target\n",
        "def numerai_score(y_true, y_pred):\n",
        "    rank_pred = y_pred.groupby(eras).apply(lambda x: x.rank(pct=True, method=\"first\"))\n",
        "    return numpy.corrcoef(y_true, rank_pred)[0,1]\n",
        "\n",
        "# It can also be convenient while working to evaluate based on the regular (pearson) correlation\n",
        "def correlation_score(y_true, y_pred):\n",
        "    return numpy.corrcoef(y_true, y_pred)[0,1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq8ufVwkc2JL"
      },
      "source": [
        "# There are 120 eras numbered from 1 to 120\n",
        "eras.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9tt3jP2dTMt"
      },
      "source": [
        "# The earlier eras are smaller, but generally each era is 4000-5000 rows\n",
        "df.groupby(eras).size().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEofP4IadTtx"
      },
      "source": [
        "# The target is discrete and takes on 5 different values\n",
        "df.groupby(target).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFUaTPSIdnPs"
      },
      "source": [
        "feature_corrs = df[features].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5cMlW-Udq20"
      },
      "source": [
        "feature_corrs.stack().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjHLi73edqzH"
      },
      "source": [
        "tdf = feature_corrs.stack()\n",
        "tdf = tdf[tdf.index.get_level_values(0) < tdf.index.get_level_values(1)]\n",
        "tdf.sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p3yZabS_svW"
      },
      "source": [
        "### The correlation can change over time\n",
        "You can see this by comparing feature correlations on the first half and second half on the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l01-z7jkdqvm"
      },
      "source": [
        "df1 = df[eras<=eras.median()]\n",
        "df2 = df[eras>eras.median()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpRgvTTAdqnn"
      },
      "source": [
        "corr1 = df1[features].corr().unstack()\n",
        "corr1 = corr1[corr1.index.get_level_values(0) < corr1.index.get_level_values(1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBU1JtVVd3uI"
      },
      "source": [
        "corr2 = df2[features].corr().unstack()\n",
        "corr2 = corr2[corr2.index.get_level_values(0) < corr2.index.get_level_values(1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5vRFjihd5e5"
      },
      "source": [
        "tdf = pandas.DataFrame({\n",
        "    \"corr1\": corr1,\n",
        "    \"corr2\": corr2,\n",
        "})\n",
        "tdf[\"corr_diff\"] = tdf.corr2 - tdf.corr1\n",
        "tdf.sort_values(by=\"corr_diff\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7SSB9_2_svY"
      },
      "source": [
        "## Some features are predictive on their own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATMtqPbSd_lT"
      },
      "source": [
        "feature_scores = {\n",
        "    feature: numerai_score(df[target], df[feature])\n",
        "    for feature in features\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh2ETQjZeBQ7"
      },
      "source": [
        "pandas.Series(feature_scores).sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXbd-OnReDTk"
      },
      "source": [
        "# Single features do not work consistently though\n",
        "by_era_correlation = pandas.Series({\n",
        "    era: numpy.corrcoef(tdf[target], tdf[\"feature_strength34\"])[0,1]\n",
        "    for era, tdf in df.groupby(eras)\n",
        "})\n",
        "by_era_correlation.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epU8yopdeFZT"
      },
      "source": [
        "# With a rolling 10 era average you can see some trends\n",
        "by_era_correlation.rolling(10).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5aU9jS4_svZ"
      },
      "source": [
        "# Gotcha: MSE looks worse than correlation out of sample\n",
        "Models will generally be overconfident, so even if they are good at ranking rows, the Mean-Squared-Error of the residuals could be larger than event the Mean-Squared-Error of the target (r-squared<0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yUX08LmeIug"
      },
      "source": [
        "df1 = df[eras<=eras.median()]\n",
        "df2 = df[eras>eras.median()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smLdOvbdeMOP"
      },
      "source": [
        "linear1 = linear_model.LinearRegression()\n",
        "linear1.fit(df1[features], df1[target])\n",
        "linear2 = linear_model.LinearRegression()\n",
        "linear2.fit(df2[features], df2[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h4vH2cuePyl"
      },
      "source": [
        "# Note in particular that the R-squared of (train_on_1, eval_on_2) is slightly negative!\n",
        "r2 = [\n",
        "    [\n",
        "        model.score(dfX[features], dfX[target])\n",
        "        for dfX in [df1, df2]\n",
        "    ]\n",
        "    for model in [linear1, linear2]\n",
        "]\n",
        "pandas.DataFrame(r2, columns=[\"eval_on_1\", \"eval_on_2\"], index=[\"train_on_1\", \"train_on_2\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmwSc9vJeR8L"
      },
      "source": [
        "# Note in particular that the correlation of (train_on_1, eval_on_2) is quite decent\n",
        "corrs = [\n",
        "    [\n",
        "        numerai_score(dfX[target], pandas.Series(model.predict(dfX[features]), index=dfX.index))\n",
        "        for dfX in [df1, df2]\n",
        "    ]\n",
        "    for model in [linear1, linear2]\n",
        "]\n",
        "pandas.DataFrame(corrs, columns=[\"eval_on_1\", \"eval_on_2\"], index=[\"train_on_1\", \"train_on_2\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jcxUwoJeURZ"
      },
      "source": [
        "xgb1 = xgboost.XGBRegressor()\n",
        "xgb1.fit(df1[features], df1[target])\n",
        "xgb2 = xgboost.XGBRegressor()\n",
        "xgb2.fit(df2[features], df2[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPQe_BSKeWCZ"
      },
      "source": [
        "r2 = [\n",
        "    [\n",
        "        model.score(dfX[features], dfX[target])\n",
        "        for dfX in [df1, df2]\n",
        "    ]\n",
        "    for model in [xgb1, xgb2]\n",
        "]\n",
        "pandas.DataFrame(r2, columns=[\"eval_on_1\", \"eval_on_2\"], index=[\"train_on_1\", \"train_on_2\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PbkppsXeXyb"
      },
      "source": [
        "corrs = [\n",
        "    [\n",
        "        numerai_score(dfX[target], pandas.Series(model.predict(dfX[features]), index=dfX.index))\n",
        "        for dfX in [df1, df2]\n",
        "    ]\n",
        "    for model in [xgb1, xgb2]\n",
        "]\n",
        "pandas.DataFrame(corrs, columns=[\"eval_on_1\", \"eval_on_2\"], index=[\"train_on_1\", \"train_on_2\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G7f24ol_svd"
      },
      "source": [
        "# Gotcha:  {0, 1} are noticeably different from {0.25, 0.75}\n",
        "This makes training a classifier one-versus-rest behave counterintuitively.\n",
        "\n",
        "Specifically, the 0-vs-rest and 1-vs-rest classifiers seem to learn how to pick out extreme targets, and their predictions are the most correlated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q3B10mr_svd",
        "outputId": "2dcc438f-58b7-46f8-e0f9-2e117e7e37ca"
      },
      "source": [
        "# Train a standard logistic regression as a classifier\n",
        "logistic = linear_model.LogisticRegression()\n",
        "logistic.fit(df[features], (df[target]*4).astype(int))\n",
        "logistic.score(df[features], (df[target]*4).astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.6/site-packages/scikit_learn-0.21.2-py3.6-linux-x86_64.egg/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.6/site-packages/scikit_learn-0.21.2-py3.6-linux-x86_64.egg/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2623872078563913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy_UKXs-_svd",
        "outputId": "f7a84fc5-8618-4cc9-aa36-7678876355cb"
      },
      "source": [
        "# The first and last class are highly correlated\n",
        "corrs=numpy.corrcoef(logistic.predict_proba(df[features]).T)\n",
        "plt.imshow(corrs, vmin=-1, vmax=1, cmap=\"RdYlGn\")\n",
        "corrs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        , -0.88666303, -0.93134968, -0.95005346,  0.87165725],\n",
              "       [-0.88666303,  1.        ,  0.8862373 ,  0.8450147 , -0.948354  ],\n",
              "       [-0.93134968,  0.8862373 ,  1.        ,  0.86030322, -0.95002613],\n",
              "       [-0.95005346,  0.8450147 ,  0.86030322,  1.        , -0.86180891],\n",
              "       [ 0.87165725, -0.948354  , -0.95002613, -0.86180891,  1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj-01qwS_sve",
        "outputId": "15140c86-6cd0-4373-a01a-c13164d3495f"
      },
      "source": [
        "# In-sample correlation is 5.4%\n",
        "preds = pandas.Series(logistic.predict_proba(df[features]).dot(logistic.classes_), index=df.index)\n",
        "numerai_score(df[target], preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.054008721403805166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO9B8aMb_sve",
        "outputId": "16f28ed6-df7e-493c-f155-6d6a76d31d32"
      },
      "source": [
        "# A standard linear model has a slightly higher correlation\n",
        "linear = linear_model.LinearRegression()\n",
        "linear.fit(df[features], df[target])\n",
        "linear.score(df[features], df[target])\n",
        "preds = pandas.Series(linear.predict(df[features]), index=df.index)\n",
        "numerai_score(df[target], preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0541934330359845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnZJGYIi_svf"
      },
      "source": [
        "# Gotcha: eras are homogenous, but different from each other\n",
        "##  Random cross-validation will look much better than cross-validating by era\n",
        "\n",
        "Even for a simple linear model, taking a random shuffle reports a correlation of 4.3%, but a time series split reports a lower score of 3.4%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BlO2XBo_svf"
      },
      "source": [
        "crossvalidators = [\n",
        "    model_selection.KFold(5),\n",
        "    model_selection.KFold(5, shuffle=True),\n",
        "    model_selection.GroupKFold(5),\n",
        "    model_selection.TimeSeriesSplit(5)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mczsSsic_svf"
      },
      "source": [
        "def correlation_score(y_true, y_pred):\n",
        "    return numpy.corrcoef(y_true, y_pred)[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-8UtSeU_svf",
        "outputId": "dcc69da9-a684-4229-fb38-81c09753ecc9"
      },
      "source": [
        "for cv in crossvalidators:\n",
        "    print(cv)\n",
        "    print(numpy.mean(\n",
        "            model_selection.cross_val_score(\n",
        "            linear_model.LinearRegression(),\n",
        "            df[features],\n",
        "            df[target],\n",
        "            cv=cv,\n",
        "            n_jobs=1,\n",
        "            groups=eras,\n",
        "            scoring=metrics.make_scorer(correlation_score, greater_is_better=True)\n",
        "        )))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KFold(n_splits=5, random_state=None, shuffle=False)\n",
            "0.03658489467457003\n",
            "\n",
            "KFold(n_splits=5, random_state=None, shuffle=True)\n",
            "0.04360860798952288\n",
            "\n",
            "GroupKFold(n_splits=5)\n",
            "0.038069863851659315\n",
            "\n",
            "TimeSeriesSplit(max_train_size=None, n_splits=5)\n",
            "0.033943838232674675\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGd1IMz6_svg"
      },
      "source": [
        "## Eras can be more or less applicable to other eras\n",
        "You can test this be splitting the eras into blocks of 10, training on each block, and evaluating on each other block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTOy9y-F_svg",
        "outputId": "982d5451-d2ec-40ea-bc4c-1dbce836f236"
      },
      "source": [
        "eras10 = (eras // 10) * 10\n",
        "eras10.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50     48186\n",
              "60     46831\n",
              "100    46107\n",
              "90     45609\n",
              "110    45070\n",
              "80     43971\n",
              "40     43439\n",
              "30     41101\n",
              "70     40403\n",
              "20     37444\n",
              "10     34600\n",
              "0      24515\n",
              "120     4532\n",
              "Name: erano, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp0S8uex_svg",
        "outputId": "bab98fa5-4cde-4a55-a7cd-44fb82459580"
      },
      "source": [
        "results10 = []\n",
        "for train_era, tdf in df[eras10<120].groupby(eras10):\n",
        "    print(train_era)\n",
        "    model = linear_model.LinearRegression()\n",
        "    model.fit(tdf[features], tdf[target])\n",
        "    for test_era, tdf in df[eras10<120].groupby(eras10):\n",
        "        results10.append([\n",
        "            train_era,\n",
        "            test_era,\n",
        "            correlation_score(tdf[target], model.predict(tdf[features]))\n",
        "        ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPhU-0H5_svg",
        "outputId": "f1181880-e98d-4494-9287-16d97bfc567a"
      },
      "source": [
        "results_df = pandas.DataFrame(\n",
        "    results10,\n",
        "    columns=[\"train_era\", \"test_era\", \"score\"]\n",
        ").pivot(index=\"train_era\", columns=\"test_era\", values=\"score\")\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>test_era</th>\n",
              "      <th>0</th>\n",
              "      <th>10</th>\n",
              "      <th>20</th>\n",
              "      <th>30</th>\n",
              "      <th>40</th>\n",
              "      <th>50</th>\n",
              "      <th>60</th>\n",
              "      <th>70</th>\n",
              "      <th>80</th>\n",
              "      <th>90</th>\n",
              "      <th>100</th>\n",
              "      <th>110</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>train_era</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.146423</td>\n",
              "      <td>0.036963</td>\n",
              "      <td>0.038011</td>\n",
              "      <td>0.034264</td>\n",
              "      <td>0.025910</td>\n",
              "      <td>0.009744</td>\n",
              "      <td>0.002876</td>\n",
              "      <td>0.037817</td>\n",
              "      <td>0.029892</td>\n",
              "      <td>0.033866</td>\n",
              "      <td>0.019426</td>\n",
              "      <td>0.004162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.042831</td>\n",
              "      <td>0.119715</td>\n",
              "      <td>0.034830</td>\n",
              "      <td>0.033846</td>\n",
              "      <td>0.035863</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.006552</td>\n",
              "      <td>0.033276</td>\n",
              "      <td>0.034233</td>\n",
              "      <td>0.038309</td>\n",
              "      <td>0.023926</td>\n",
              "      <td>0.014590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.043862</td>\n",
              "      <td>0.040213</td>\n",
              "      <td>0.123154</td>\n",
              "      <td>0.036627</td>\n",
              "      <td>0.023993</td>\n",
              "      <td>0.009394</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.027858</td>\n",
              "      <td>0.016508</td>\n",
              "      <td>0.035827</td>\n",
              "      <td>0.021159</td>\n",
              "      <td>0.007964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.043145</td>\n",
              "      <td>0.040596</td>\n",
              "      <td>0.043468</td>\n",
              "      <td>0.108705</td>\n",
              "      <td>0.037635</td>\n",
              "      <td>0.023734</td>\n",
              "      <td>0.007650</td>\n",
              "      <td>0.046880</td>\n",
              "      <td>0.023781</td>\n",
              "      <td>0.039757</td>\n",
              "      <td>0.010581</td>\n",
              "      <td>0.010506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.039806</td>\n",
              "      <td>0.044743</td>\n",
              "      <td>0.029963</td>\n",
              "      <td>0.037634</td>\n",
              "      <td>0.098834</td>\n",
              "      <td>0.014143</td>\n",
              "      <td>0.007886</td>\n",
              "      <td>0.035910</td>\n",
              "      <td>0.028617</td>\n",
              "      <td>0.031637</td>\n",
              "      <td>0.008431</td>\n",
              "      <td>0.019615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.017756</td>\n",
              "      <td>0.006481</td>\n",
              "      <td>0.011675</td>\n",
              "      <td>0.025233</td>\n",
              "      <td>0.014928</td>\n",
              "      <td>0.106223</td>\n",
              "      <td>0.009696</td>\n",
              "      <td>0.017742</td>\n",
              "      <td>0.002873</td>\n",
              "      <td>0.018168</td>\n",
              "      <td>0.014381</td>\n",
              "      <td>0.016401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0.011671</td>\n",
              "      <td>0.016350</td>\n",
              "      <td>0.006663</td>\n",
              "      <td>0.009488</td>\n",
              "      <td>0.009990</td>\n",
              "      <td>0.009639</td>\n",
              "      <td>0.104522</td>\n",
              "      <td>0.017671</td>\n",
              "      <td>0.006228</td>\n",
              "      <td>0.003792</td>\n",
              "      <td>0.004541</td>\n",
              "      <td>0.010718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.039278</td>\n",
              "      <td>0.036966</td>\n",
              "      <td>0.028348</td>\n",
              "      <td>0.034946</td>\n",
              "      <td>0.025443</td>\n",
              "      <td>0.006190</td>\n",
              "      <td>0.010160</td>\n",
              "      <td>0.129820</td>\n",
              "      <td>0.032531</td>\n",
              "      <td>0.035526</td>\n",
              "      <td>0.019330</td>\n",
              "      <td>0.008851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.038294</td>\n",
              "      <td>0.038707</td>\n",
              "      <td>0.017229</td>\n",
              "      <td>0.020259</td>\n",
              "      <td>0.021633</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.003182</td>\n",
              "      <td>0.034221</td>\n",
              "      <td>0.108331</td>\n",
              "      <td>0.030059</td>\n",
              "      <td>0.017398</td>\n",
              "      <td>0.006075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.032509</td>\n",
              "      <td>0.035588</td>\n",
              "      <td>0.031530</td>\n",
              "      <td>0.032199</td>\n",
              "      <td>0.020467</td>\n",
              "      <td>0.009432</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.032199</td>\n",
              "      <td>0.022233</td>\n",
              "      <td>0.120684</td>\n",
              "      <td>0.022538</td>\n",
              "      <td>0.014140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.026869</td>\n",
              "      <td>0.021840</td>\n",
              "      <td>0.022120</td>\n",
              "      <td>0.007464</td>\n",
              "      <td>0.004211</td>\n",
              "      <td>0.014896</td>\n",
              "      <td>0.004660</td>\n",
              "      <td>0.023715</td>\n",
              "      <td>0.013903</td>\n",
              "      <td>0.027636</td>\n",
              "      <td>0.104936</td>\n",
              "      <td>0.013309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0.004968</td>\n",
              "      <td>0.022504</td>\n",
              "      <td>0.009028</td>\n",
              "      <td>0.011521</td>\n",
              "      <td>0.019793</td>\n",
              "      <td>0.014488</td>\n",
              "      <td>0.007307</td>\n",
              "      <td>0.015275</td>\n",
              "      <td>0.008017</td>\n",
              "      <td>0.023518</td>\n",
              "      <td>0.013382</td>\n",
              "      <td>0.093638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "test_era        0         10        20        30        40        50   \\\n",
              "train_era                                                               \n",
              "0          0.146423  0.036963  0.038011  0.034264  0.025910  0.009744   \n",
              "10         0.042831  0.119715  0.034830  0.033846  0.035863  0.003364   \n",
              "20         0.043862  0.040213  0.123154  0.036627  0.023993  0.009394   \n",
              "30         0.043145  0.040596  0.043468  0.108705  0.037635  0.023734   \n",
              "40         0.039806  0.044743  0.029963  0.037634  0.098834  0.014143   \n",
              "50         0.017756  0.006481  0.011675  0.025233  0.014928  0.106223   \n",
              "60         0.011671  0.016350  0.006663  0.009488  0.009990  0.009639   \n",
              "70         0.039278  0.036966  0.028348  0.034946  0.025443  0.006190   \n",
              "80         0.038294  0.038707  0.017229  0.020259  0.021633  0.000531   \n",
              "90         0.032509  0.035588  0.031530  0.032199  0.020467  0.009432   \n",
              "100        0.026869  0.021840  0.022120  0.007464  0.004211  0.014896   \n",
              "110        0.004968  0.022504  0.009028  0.011521  0.019793  0.014488   \n",
              "\n",
              "test_era        60        70        80        90        100       110  \n",
              "train_era                                                              \n",
              "0          0.002876  0.037817  0.029892  0.033866  0.019426  0.004162  \n",
              "10         0.006552  0.033276  0.034233  0.038309  0.023926  0.014590  \n",
              "20         0.000732  0.027858  0.016508  0.035827  0.021159  0.007964  \n",
              "30         0.007650  0.046880  0.023781  0.039757  0.010581  0.010506  \n",
              "40         0.007886  0.035910  0.028617  0.031637  0.008431  0.019615  \n",
              "50         0.009696  0.017742  0.002873  0.018168  0.014381  0.016401  \n",
              "60         0.104522  0.017671  0.006228  0.003792  0.004541  0.010718  \n",
              "70         0.010160  0.129820  0.032531  0.035526  0.019330  0.008851  \n",
              "80         0.003182  0.034221  0.108331  0.030059  0.017398  0.006075  \n",
              "90         0.000152  0.032199  0.022233  0.120684  0.022538  0.014140  \n",
              "100        0.004660  0.023715  0.013903  0.027636  0.104936  0.013309  \n",
              "110        0.007307  0.015275  0.008017  0.023518  0.013382  0.093638  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGAI62pt_svh",
        "outputId": "6b5cc912-62e4-4c03-e718-780317fcceb7"
      },
      "source": [
        "# Each row here is the training block of eras, each column is a testing block of eras.\n",
        "# Note that there is a period in the middle that does not seem to be relevant to other eras, and the\n",
        "#  overall performance seems to decrease a bit over time.\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(results_df, vmin=-0.04, vmax=0.04, cmap=\"RdYlGn\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6732b723c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fA7k8Gci_svh"
      },
      "source": [
        "Here is an advanced paper that talks about generalization.\n",
        "Eras can be thought about in the same way that \"distributions\" or \"environments\" are talked about here\n",
        "https://arxiv.org/pdf/1907.02893.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L7eh8JQ_svh"
      },
      "source": [
        "## Gotcha: Since the signal-to-noise ratio is so low, models can take many more iterations than expected, and have scarily high in-sample performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKF6PS2O_svi",
        "outputId": "2d0eeacc-3b7e-4afc-ca0e-fff6147cd749"
      },
      "source": [
        "def our_score(preds, dtrain):\n",
        "    return \"score\", -numpy.corrcoef(preds, dtrain.get_label())[0,1]\n",
        "\n",
        "dtrain = xgboost.DMatrix(df1[features], df1[target])\n",
        "dtest = xgboost.DMatrix(df2[features], df2[target])\n",
        "dall = xgboost.DMatrix(df[features], df[target])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.6/site-packages/xgboost-0.90-py3.6-linux-x86_64.egg/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
            "  if getattr(data, 'base', None) is not None and \\\n",
            "/home/ubuntu/.local/lib/python3.6/site-packages/xgboost-0.90-py3.6-linux-x86_64.egg/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
            "  data.base is not None and isinstance(data, np.ndarray) \\\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Bte_kEXj_svi",
        "outputId": "9d5b1c38-e050-43a6-d0e4-22f9f5a83e43"
      },
      "source": [
        "param = {\n",
        "    'max_depth':3,\n",
        "    'eta':0.1,\n",
        "    'silent':1,\n",
        "    'objective':'reg:linear',\n",
        "    'eval_metric':'rmse',\n",
        "    'nthread': -1,\n",
        "}\n",
        "evals_result = {}\n",
        "bst = xgboost.train(\n",
        "    params=param,\n",
        "    dtrain=dtrain,\n",
        "    feval=our_score,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
        "    evals_result=evals_result,\n",
        "    verbose_eval=10,\n",
        ")\n",
        "\n",
        "(0.5 - 0.57*pandas.DataFrame({k: v['score'] for k,v in evals_result.items()})).plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-rmse:0.35289\ttest-rmse:0.353029\ttrain-score:-0.036672\ttest-score:-0.018971\n",
            "[10]\ttrain-rmse:0.352604\ttest-rmse:0.352907\ttrain-score:-0.060901\ttest-score:-0.030237\n",
            "[20]\ttrain-rmse:0.352406\ttest-rmse:0.352849\ttrain-score:-0.070733\ttest-score:-0.033376\n",
            "[30]\ttrain-rmse:0.352255\ttest-rmse:0.352824\ttrain-score:-0.077785\ttest-score:-0.034502\n",
            "[40]\ttrain-rmse:0.352109\ttest-rmse:0.352797\ttrain-score:-0.083381\ttest-score:-0.036086\n",
            "[50]\ttrain-rmse:0.351989\ttest-rmse:0.35279\ttrain-score:-0.088237\ttest-score:-0.036645\n",
            "[60]\ttrain-rmse:0.351874\ttest-rmse:0.352778\ttrain-score:-0.092579\ttest-score:-0.037602\n",
            "[70]\ttrain-rmse:0.351769\ttest-rmse:0.352774\ttrain-score:-0.096485\ttest-score:-0.038273\n",
            "[80]\ttrain-rmse:0.35167\ttest-rmse:0.352771\ttrain-score:-0.099683\ttest-score:-0.038777\n",
            "[90]\ttrain-rmse:0.351575\ttest-rmse:0.352775\ttrain-score:-0.103451\ttest-score:-0.038894\n",
            "[100]\ttrain-rmse:0.35148\ttest-rmse:0.352776\ttrain-score:-0.107079\ttest-score:-0.039126\n",
            "[110]\ttrain-rmse:0.351393\ttest-rmse:0.352776\ttrain-score:-0.11002\ttest-score:-0.039627\n",
            "[120]\ttrain-rmse:0.351305\ttest-rmse:0.352785\ttrain-score:-0.112903\ttest-score:-0.039542\n",
            "[130]\ttrain-rmse:0.35122\ttest-rmse:0.352789\ttrain-score:-0.115709\ttest-score:-0.039659\n",
            "[140]\ttrain-rmse:0.351144\ttest-rmse:0.352803\ttrain-score:-0.118515\ttest-score:-0.039389\n",
            "[150]\ttrain-rmse:0.351061\ttest-rmse:0.352807\ttrain-score:-0.120926\ttest-score:-0.039736\n",
            "[160]\ttrain-rmse:0.350985\ttest-rmse:0.352816\ttrain-score:-0.123161\ttest-score:-0.039757\n",
            "[170]\ttrain-rmse:0.350907\ttest-rmse:0.352826\ttrain-score:-0.125857\ttest-score:-0.039733\n",
            "[180]\ttrain-rmse:0.350827\ttest-rmse:0.352826\ttrain-score:-0.128098\ttest-score:-0.040137\n",
            "[190]\ttrain-rmse:0.350751\ttest-rmse:0.352831\ttrain-score:-0.130531\ttest-score:-0.040231\n",
            "[200]\ttrain-rmse:0.350673\ttest-rmse:0.352831\ttrain-score:-0.132826\ttest-score:-0.040608\n",
            "[210]\ttrain-rmse:0.350604\ttest-rmse:0.352845\ttrain-score:-0.134819\ttest-score:-0.040475\n",
            "[220]\ttrain-rmse:0.350537\ttest-rmse:0.352847\ttrain-score:-0.136791\ttest-score:-0.040637\n",
            "[230]\ttrain-rmse:0.350476\ttest-rmse:0.352851\ttrain-score:-0.138681\ttest-score:-0.040764\n",
            "[240]\ttrain-rmse:0.350404\ttest-rmse:0.352859\ttrain-score:-0.140549\ttest-score:-0.040961\n",
            "[250]\ttrain-rmse:0.350337\ttest-rmse:0.352864\ttrain-score:-0.142532\ttest-score:-0.041083\n",
            "[260]\ttrain-rmse:0.35027\ttest-rmse:0.352863\ttrain-score:-0.144547\ttest-score:-0.041357\n",
            "[270]\ttrain-rmse:0.350205\ttest-rmse:0.35287\ttrain-score:-0.146406\ttest-score:-0.041408\n",
            "[280]\ttrain-rmse:0.35014\ttest-rmse:0.352882\ttrain-score:-0.148137\ttest-score:-0.04123\n",
            "[290]\ttrain-rmse:0.350078\ttest-rmse:0.352895\ttrain-score:-0.149793\ttest-score:-0.041042\n",
            "[300]\ttrain-rmse:0.350016\ttest-rmse:0.352909\ttrain-score:-0.151378\ttest-score:-0.040826\n",
            "[310]\ttrain-rmse:0.349953\ttest-rmse:0.352918\ttrain-score:-0.152825\ttest-score:-0.040895\n",
            "[320]\ttrain-rmse:0.349893\ttest-rmse:0.352924\ttrain-score:-0.154626\ttest-score:-0.040873\n",
            "[330]\ttrain-rmse:0.34983\ttest-rmse:0.352933\ttrain-score:-0.156246\ttest-score:-0.040781\n",
            "[340]\ttrain-rmse:0.34977\ttest-rmse:0.352941\ttrain-score:-0.157914\ttest-score:-0.040768\n",
            "[350]\ttrain-rmse:0.34971\ttest-rmse:0.352946\ttrain-score:-0.159419\ttest-score:-0.040972\n",
            "[360]\ttrain-rmse:0.349644\ttest-rmse:0.352961\ttrain-score:-0.160967\ttest-score:-0.040753\n",
            "[370]\ttrain-rmse:0.349589\ttest-rmse:0.352969\ttrain-score:-0.162375\ttest-score:-0.040752\n",
            "[380]\ttrain-rmse:0.349524\ttest-rmse:0.352976\ttrain-score:-0.164134\ttest-score:-0.0407\n",
            "[390]\ttrain-rmse:0.349463\ttest-rmse:0.352984\ttrain-score:-0.16552\ttest-score:-0.040762\n",
            "[400]\ttrain-rmse:0.349405\ttest-rmse:0.35299\ttrain-score:-0.167064\ttest-score:-0.040765\n",
            "[410]\ttrain-rmse:0.349343\ttest-rmse:0.352991\ttrain-score:-0.168644\ttest-score:-0.040964\n",
            "[420]\ttrain-rmse:0.349281\ttest-rmse:0.352999\ttrain-score:-0.1703\ttest-score:-0.040965\n",
            "[430]\ttrain-rmse:0.349221\ttest-rmse:0.353008\ttrain-score:-0.171884\ttest-score:-0.040924\n",
            "[440]\ttrain-rmse:0.349163\ttest-rmse:0.353019\ttrain-score:-0.173448\ttest-score:-0.040691\n",
            "[450]\ttrain-rmse:0.349104\ttest-rmse:0.353026\ttrain-score:-0.174742\ttest-score:-0.040719\n",
            "[460]\ttrain-rmse:0.349048\ttest-rmse:0.353043\ttrain-score:-0.176047\ttest-score:-0.040515\n",
            "[470]\ttrain-rmse:0.348994\ttest-rmse:0.353047\ttrain-score:-0.177304\ttest-score:-0.040628\n",
            "[480]\ttrain-rmse:0.348935\ttest-rmse:0.35305\ttrain-score:-0.17878\ttest-score:-0.040625\n",
            "[490]\ttrain-rmse:0.348881\ttest-rmse:0.353052\ttrain-score:-0.180321\ttest-score:-0.040779\n",
            "[500]\ttrain-rmse:0.348828\ttest-rmse:0.353064\ttrain-score:-0.181524\ttest-score:-0.040672\n",
            "[510]\ttrain-rmse:0.348773\ttest-rmse:0.353077\ttrain-score:-0.182828\ttest-score:-0.040411\n",
            "[520]\ttrain-rmse:0.348719\ttest-rmse:0.35308\ttrain-score:-0.184195\ttest-score:-0.0406\n",
            "[530]\ttrain-rmse:0.348663\ttest-rmse:0.353082\ttrain-score:-0.185463\ttest-score:-0.04076\n",
            "[540]\ttrain-rmse:0.348611\ttest-rmse:0.353084\ttrain-score:-0.186865\ttest-score:-0.040852\n",
            "[550]\ttrain-rmse:0.348557\ttest-rmse:0.353097\ttrain-score:-0.188067\ttest-score:-0.040715\n",
            "[560]\ttrain-rmse:0.348504\ttest-rmse:0.353104\ttrain-score:-0.18931\ttest-score:-0.040648\n",
            "[570]\ttrain-rmse:0.348451\ttest-rmse:0.353117\ttrain-score:-0.190356\ttest-score:-0.040564\n",
            "[580]\ttrain-rmse:0.348394\ttest-rmse:0.353123\ttrain-score:-0.19196\ttest-score:-0.040462\n",
            "[590]\ttrain-rmse:0.348344\ttest-rmse:0.35313\ttrain-score:-0.193121\ttest-score:-0.040481\n",
            "[600]\ttrain-rmse:0.348292\ttest-rmse:0.353142\ttrain-score:-0.193977\ttest-score:-0.040508\n",
            "[610]\ttrain-rmse:0.348235\ttest-rmse:0.353146\ttrain-score:-0.195221\ttest-score:-0.040646\n",
            "[620]\ttrain-rmse:0.348188\ttest-rmse:0.353153\ttrain-score:-0.196006\ttest-score:-0.040727\n",
            "[630]\ttrain-rmse:0.348141\ttest-rmse:0.353165\ttrain-score:-0.197121\ttest-score:-0.040569\n",
            "[640]\ttrain-rmse:0.348087\ttest-rmse:0.353168\ttrain-score:-0.198471\ttest-score:-0.040639\n",
            "[650]\ttrain-rmse:0.348036\ttest-rmse:0.353176\ttrain-score:-0.199684\ttest-score:-0.040547\n",
            "[660]\ttrain-rmse:0.347984\ttest-rmse:0.353186\ttrain-score:-0.200591\ttest-score:-0.040485\n",
            "[670]\ttrain-rmse:0.347934\ttest-rmse:0.353189\ttrain-score:-0.201821\ttest-score:-0.040502\n",
            "[680]\ttrain-rmse:0.347882\ttest-rmse:0.353195\ttrain-score:-0.203078\ttest-score:-0.040522\n",
            "[690]\ttrain-rmse:0.347835\ttest-rmse:0.353195\ttrain-score:-0.204161\ttest-score:-0.040623\n",
            "[700]\ttrain-rmse:0.347782\ttest-rmse:0.353202\ttrain-score:-0.205183\ttest-score:-0.040715\n",
            "[710]\ttrain-rmse:0.347732\ttest-rmse:0.353208\ttrain-score:-0.206217\ttest-score:-0.040714\n",
            "[720]\ttrain-rmse:0.347682\ttest-rmse:0.353217\ttrain-score:-0.207257\ttest-score:-0.040665\n",
            "[730]\ttrain-rmse:0.347632\ttest-rmse:0.353234\ttrain-score:-0.208297\ttest-score:-0.040383\n",
            "[740]\ttrain-rmse:0.347586\ttest-rmse:0.353241\ttrain-score:-0.209288\ttest-score:-0.040378\n",
            "[750]\ttrain-rmse:0.347537\ttest-rmse:0.353253\ttrain-score:-0.210239\ttest-score:-0.040291\n",
            "[760]\ttrain-rmse:0.347484\ttest-rmse:0.353261\ttrain-score:-0.211296\ttest-score:-0.040235\n",
            "[770]\ttrain-rmse:0.347435\ttest-rmse:0.353268\ttrain-score:-0.212326\ttest-score:-0.04031\n",
            "[780]\ttrain-rmse:0.347387\ttest-rmse:0.353278\ttrain-score:-0.213435\ttest-score:-0.040124\n",
            "[790]\ttrain-rmse:0.347338\ttest-rmse:0.353287\ttrain-score:-0.214471\ttest-score:-0.040097\n",
            "[800]\ttrain-rmse:0.347293\ttest-rmse:0.353291\ttrain-score:-0.21549\ttest-score:-0.040079\n",
            "[810]\ttrain-rmse:0.347241\ttest-rmse:0.353298\ttrain-score:-0.216715\ttest-score:-0.040053\n",
            "[820]\ttrain-rmse:0.347193\ttest-rmse:0.353305\ttrain-score:-0.217609\ttest-score:-0.04\n",
            "[830]\ttrain-rmse:0.347147\ttest-rmse:0.353319\ttrain-score:-0.218521\ttest-score:-0.039874\n",
            "[840]\ttrain-rmse:0.347101\ttest-rmse:0.353329\ttrain-score:-0.219304\ttest-score:-0.039863\n",
            "[850]\ttrain-rmse:0.347051\ttest-rmse:0.353333\ttrain-score:-0.220362\ttest-score:-0.039835\n",
            "[860]\ttrain-rmse:0.347003\ttest-rmse:0.35334\ttrain-score:-0.221392\ttest-score:-0.039846\n",
            "[870]\ttrain-rmse:0.346957\ttest-rmse:0.353343\ttrain-score:-0.222233\ttest-score:-0.040043\n",
            "[880]\ttrain-rmse:0.346911\ttest-rmse:0.353349\ttrain-score:-0.223018\ttest-score:-0.040064\n",
            "[890]\ttrain-rmse:0.34686\ttest-rmse:0.353355\ttrain-score:-0.224135\ttest-score:-0.040005\n",
            "[900]\ttrain-rmse:0.346813\ttest-rmse:0.353365\ttrain-score:-0.224935\ttest-score:-0.04003\n",
            "[910]\ttrain-rmse:0.346767\ttest-rmse:0.353372\ttrain-score:-0.225785\ttest-score:-0.040029\n",
            "[920]\ttrain-rmse:0.346718\ttest-rmse:0.353377\ttrain-score:-0.22689\ttest-score:-0.040003\n",
            "[930]\ttrain-rmse:0.346673\ttest-rmse:0.353385\ttrain-score:-0.2276\ttest-score:-0.04013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[940]\ttrain-rmse:0.346621\ttest-rmse:0.353394\ttrain-score:-0.228575\ttest-score:-0.040005\n",
            "[950]\ttrain-rmse:0.346571\ttest-rmse:0.353398\ttrain-score:-0.229612\ttest-score:-0.040051\n",
            "[960]\ttrain-rmse:0.346527\ttest-rmse:0.353403\ttrain-score:-0.230449\ttest-score:-0.040087\n",
            "[970]\ttrain-rmse:0.346479\ttest-rmse:0.353415\ttrain-score:-0.231366\ttest-score:-0.039987\n",
            "[980]\ttrain-rmse:0.346428\ttest-rmse:0.353426\ttrain-score:-0.232327\ttest-score:-0.039943\n",
            "[990]\ttrain-rmse:0.34638\ttest-rmse:0.353433\ttrain-score:-0.233212\ttest-score:-0.039949\n",
            "[999]\ttrain-rmse:0.34634\ttest-rmse:0.353429\ttrain-score:-0.234118\ttest-score:-0.040008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6730428400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga66THuE_svi",
        "outputId": "f464ec33-25b1-4e54-f617-19df94d90961"
      },
      "source": [
        "(-pandas.DataFrame({k: v['score'] for k,v in evals_result.items()})).plot(ylim=[0,0.045])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f673315a160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BXzsm4l_svj"
      },
      "source": [
        "# The results are sensitive to the choice of parameters, which should be picked through cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMy5v4Ni_svj"
      },
      "source": [
        "df1 = df[eras<=eras.median()]\n",
        "df2 = df[eras>eras.median()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BuKBy07_svj"
      },
      "source": [
        "models = [\n",
        "    linear_model.LinearRegression(),\n",
        "] + [\n",
        "    linear_model.ElasticNet(alpha=alpha)\n",
        "    for alpha in [0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]\n",
        "] + [\n",
        "    xgboost.XGBRegressor(n_jobs=-1),\n",
        "    xgboost.XGBRegressor(n_jobs=-1, learning_rate=0.01, n_estimators=1000),\n",
        "    xgboost.XGBRegressor(n_jobs=-1, colsample_bytree=0.1, learning_rate=0.01, n_estimators=1000),\n",
        "    xgboost.XGBRegressor(n_jobs=-1, colsample_bytree=0.1, learning_rate=0.01, n_estimators=1000, max_depth=5),\n",
        "    xgboost.XGBRegressor(n_jobs=-1, colsample_bytree=0.1, learning_rate=0.001, n_estimators=10000, max_depth=5),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "6vI8rOvN_svk",
        "outputId": "5ee6cfd7-6bd2-489d-fe13-5eaaf265d3d4"
      },
      "source": [
        "for model in models:\n",
        "    print(\" -- \", model)   \n",
        "    model.fit(df1[features], df1[target])\n",
        "    outsample = numerai_score(df2[target], pandas.Series(model.predict(df2[features]), index=df2.index))\n",
        "    insample = numerai_score(df1[target], pandas.Series(model.predict(df1[features]), index=df1.index))\n",
        "    print(\n",
        "        f\"outsample: {outsample}, insample: {insample}\"\n",
        "    )\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " --  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
            "outsample: 0.03099809450665611, insample: 0.06588644281475838\n",
            "\n",
            " --  ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.001706090379087756, insample: 0.003084687498966189\n",
            "\n",
            " --  ElasticNet(alpha=0.005, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.001706090379087756, insample: 0.003084687498966189\n",
            "\n",
            " --  ElasticNet(alpha=0.002, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.02296653186992195, insample: 0.044022473132496145\n",
            "\n",
            " --  ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.02989318456886029, insample: 0.05184159813561343\n",
            "\n",
            " --  ElasticNet(alpha=0.0005, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.032645669091276176, insample: 0.057053818225275\n",
            "\n",
            " --  ElasticNet(alpha=0.0002, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.033010509057742096, insample: 0.061666479426053425\n",
            "\n",
            " --  ElasticNet(alpha=0.0001, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.03240338370856842, insample: 0.0635220041576513\n",
            "\n",
            " --  ElasticNet(alpha=5e-05, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.031880042570450504, insample: 0.06485026703936253\n",
            "\n",
            " --  ElasticNet(alpha=2e-05, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.03143702673815047, insample: 0.06560409030218367\n",
            "\n",
            " --  ElasticNet(alpha=1e-05, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "outsample: 0.031247308413302647, insample: 0.06579475006648515\n",
            "\n",
            " --  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
            "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
            "             n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n",
            "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "             silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.6/site-packages/xgboost-0.90-py3.6-linux-x86_64.egg/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
            "  if getattr(data, 'base', None) is not None and \\\n",
            "/home/ubuntu/.local/lib/python3.6/site-packages/xgboost-0.90-py3.6-linux-x86_64.egg/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
            "  data.base is not None and isinstance(data, np.ndarray) \\\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[18:59:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "outsample: 0.03822459435002261, insample: 0.10372868743447641\n",
            "\n",
            " --  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
            "             max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
            "             n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n",
            "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "             silent=None, subsample=1, verbosity=1)\n",
            "[19:02:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "outsample: 0.0387815520050224, insample: 0.1055617044604215\n",
            "\n",
            " --  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.1, gamma=0,\n",
            "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
            "             max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
            "             n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n",
            "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "             silent=None, subsample=1, verbosity=1)\n",
            "[19:28:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "outsample: 0.040825170763223534, insample: 0.09738860291404573\n",
            "\n",
            " --  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.1, gamma=0,\n",
            "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
            "             max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
            "             n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n",
            "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "             silent=None, subsample=1, verbosity=1)\n",
            "[19:36:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "outsample: 0.043311203339638234, insample: 0.1998400250413849\n",
            "\n",
            " --  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.1, gamma=0,\n",
            "             importance_type='gain', learning_rate=0.001, max_delta_step=0,\n",
            "             max_depth=5, min_child_weight=1, missing=None, n_estimators=10000,\n",
            "             n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n",
            "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "             silent=None, subsample=1, verbosity=1)\n",
            "[19:46:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "outsample: 0.04398737850050243, insample: 0.202515264502208\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38xifdgH_svk"
      },
      "source": [
        "## Gotcha: Models with large exposures to individual features tend to perform poorly or inconsistently out of sample ## \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIH5nau-_svk"
      },
      "source": [
        "import numpy as np\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeKcsAbK_svl",
        "outputId": "80ef2683-0af1-4a41-c641-fd6733e947b0"
      },
      "source": [
        "# Train a standard xgboost on half the train eras\n",
        "xgb = xgboost.XGBRegressor(n_estimators=1000, max_depth=5, learning_rate=0.01, n_jobs=-1)\n",
        "xgb.fit(df1[features], df1[target])\n",
        "xgb_preds = xgb.predict(df2[features])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[21:32:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twJzjbB6_svl"
      },
      "source": [
        "### Our predictions have correlation > 0.2 in either direction for some single features!\n",
        "Sure hope those features continue to act as they have in the past!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTIneADK_svl",
        "outputId": "5de6a0ff-3707-4ac2-a9e1-b7da865996d6"
      },
      "source": [
        "corr_list = []\n",
        "for feature in features:\n",
        "    corr_list.append(numpy.corrcoef(df2[feature], xgb_preds)[0,1])\n",
        "corr_series = pandas.Series(corr_list, index=features)\n",
        "corr_series.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    310.000000\n",
              "mean       0.046897\n",
              "std        0.084379\n",
              "min       -0.271691\n",
              "25%        0.007401\n",
              "50%        0.044329\n",
              "75%        0.101984\n",
              "max        0.220213\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MylAWzw_svl"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def _neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df[columns]\n",
        "    exposures = df[by].values\n",
        "    scores = scores - proportion * exposures.dot(numpy.linalg.pinv(exposures).dot(scores))\n",
        "    return scores / scores.std(ddof=0)\n",
        "def _normalize(df):\n",
        "    X = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return scipy.stats.norm.ppf(X)\n",
        "def normalize_and_neutralize(df, columns, by, proportion=1.0):\n",
        "    # Convert the scores to a normal distribution\n",
        "    df[columns] = _normalize(df[columns])\n",
        "    df[columns] = _neutralize(df, columns, by, proportion)\n",
        "    return df[columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5BPkj-R_svm",
        "outputId": "29a21755-a2a7-4fd4-a9d6-98436e2420c1"
      },
      "source": [
        "df2[\"preds\"] = xgb_preds\n",
        "df2[\"preds_neutralized\"] = df2.groupby(\"era\").apply(\n",
        "    lambda x: normalize_and_neutralize(x, [\"preds\"], features, 0.5) # neutralize by 50% within each era\n",
        ")\n",
        "scaler = MinMaxScaler()\n",
        "df2[\"preds_neutralized\"] = scaler.fit_transform(df2[[\"preds_neutralized\"]]) # transform back to 0-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdvTF_qE_svm"
      },
      "source": [
        "### Now our single feature exposures are much smaller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLObidLE_svm"
      },
      "source": [
        "corr_list2 = []\n",
        "for feature in features:\n",
        "    corr_list2.append(numpy.corrcoef(df2[feature], df2[\"preds_neutralized\"])[0,1])\n",
        "corr_series2 = pandas.Series(corr_list2, index=features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZvYMuhe_svm",
        "outputId": "5117a28c-6d87-4d81-db0c-6bcc20520fa8"
      },
      "source": [
        "corr_series2.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    310.000000\n",
              "mean       0.036216\n",
              "std        0.055772\n",
              "min       -0.174678\n",
              "25%        0.009870\n",
              "50%        0.034649\n",
              "75%        0.073136\n",
              "max        0.150736\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NWK-PIF_svn"
      },
      "source": [
        "### Our overall score goes down, but the scores are more consistent than before. This leads to a higher sharpe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uUGfltZ_svn",
        "outputId": "f070de16-a41d-4199-8c25-b918a353e10a"
      },
      "source": [
        "unbalanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"preds\"], d[target])[0,1])\n",
        "balanced_scores_per_era = df2.groupby(\"era\").apply(lambda d: np.corrcoef(d[\"preds_neutralized\"], d[target])[0,1])\n",
        "\n",
        "print(f\"score for high feature exposure: {unbalanced_scores_per_era.mean()}\")\n",
        "print(f\"score for balanced feature expo: {balanced_scores_per_era.mean()}\")\n",
        "\n",
        "print(f\"std for high feature exposure: {unbalanced_scores_per_era.std(ddof=0)}\")\n",
        "print(f\"std for balanced feature expo: {balanced_scores_per_era.std(ddof=0)}\")\n",
        "\n",
        "print(f\"sharpe for high feature exposure: {unbalanced_scores_per_era.mean()/unbalanced_scores_per_era.std(ddof=0)}\")\n",
        "print(f\"sharpe for balanced feature expo: {balanced_scores_per_era.mean()/balanced_scores_per_era.std(ddof=0)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score for high feature exposure: 0.0413834154777531\n",
            "score for balanced feature expo: 0.03633214642701158\n",
            "std for high feature exposure: 0.04021166505490904\n",
            "std for balanced feature expo: 0.0318307699959389\n",
            "sharpe for high feature exposure: 1.0291395648810868\n",
            "sharpe for balanced feature expo: 1.1414158825453165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPM7SocC_svn",
        "outputId": "325c6a77-f69e-4bf7-d525-65c91d7134f8"
      },
      "source": [
        "balanced_scores_per_era.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    56.000000\n",
              "mean      0.036332\n",
              "std       0.031831\n",
              "min      -0.051360\n",
              "25%       0.015198\n",
              "50%       0.036021\n",
              "75%       0.058698\n",
              "max       0.101365\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi56R8i7_svn",
        "outputId": "d06df328-a17f-45c5-df7c-f2dcebbbc132"
      },
      "source": [
        "unbalanced_scores_per_era.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    56.000000\n",
              "mean      0.041383\n",
              "std       0.040212\n",
              "min      -0.077937\n",
              "25%       0.014694\n",
              "50%       0.044316\n",
              "75%       0.066439\n",
              "max       0.125521\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzwQrJU_svo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}